{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f628d5da-5185-4fe4-8281-5b583889103a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59b5724d-8422-44bf-8d8f-0a9530b394eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Data Shape: (45019243, 40)\n"
     ]
    }
   ],
   "source": [
    "# Path to folder with all CSVs\n",
    "folder_path = '/Users/user/Downloads/Table/downloaded_csvs_merge'\n",
    "\n",
    "# Find all CSV files\n",
    "csv_files = glob(os.path.join(folder_path, '*.csv'))\n",
    "\n",
    "# Load and concatenate all CSVs\n",
    "df_list = []\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df_chunk = pd.read_csv(file)\n",
    "        df_list.append(df_chunk)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {file}: {e}\")\n",
    "\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "print(\"Combined Data Shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f00eca8-6f5d-4469-b1cb-382338a5cb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining features: 29\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.9\n",
    "to_drop = []\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].isnull().mean() > threshold or (df[col] == 0).mean() > threshold:\n",
    "        to_drop.append(col)\n",
    "\n",
    "df = df.drop(columns=to_drop)\n",
    "print(f\"Remaining features: {df.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e583673-4c41-4fcc-b898-242fe810700b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after dropping nulls: (45018564, 29)\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(\"Shape after dropping nulls:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbc82f0b-9c3d-4814-a496-cc93753fb1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "DDOS-ICMP_FLOOD            6893203\n",
      "DDOS-UDP_FLOOD             5180997\n",
      "DDOS-TCP_FLOOD             4306058\n",
      "DDOS-PSHACK_FLOOD          3920337\n",
      "DDOS-SYN_FLOOD             3886105\n",
      "DDOS-RSTFINFLOOD           3872787\n",
      "DDOS-SYNONYMOUSIP_FLOOD    3445630\n",
      "DOS-UDP_FLOOD              3177280\n",
      "DOS-TCP_FLOOD              2558232\n",
      "DOS-SYN_FLOOD              1942159\n",
      "BENIGN                     1051313\n",
      "MIRAI-GREETH_FLOOD          949325\n",
      "MIRAI-UDPPLAIN              852632\n",
      "MIRAI-GREIP_FLOOD           719604\n",
      "DDOS-ICMP_FRAGMENTATION     433115\n",
      "VULNERABILITYSCAN           357579\n",
      "MITM-ARPSPOOFING            294451\n",
      "DDOS-UDP_FRAGMENTATION      274881\n",
      "DDOS-ACK_FRAGMENTATION      272767\n",
      "DNS_SPOOFING                171463\n",
      "RECON-HOSTDISCOVERY         128676\n",
      "RECON-OSSCAN                 93966\n",
      "RECON-PORTSCAN               78729\n",
      "DOS-HTTP_FLOOD               68798\n",
      "DDOS-HTTP_FLOOD              27597\n",
      "DDOS-SLOWLORIS               22399\n",
      "DICTIONARYBRUTEFORCE         12522\n",
      "BROWSERHIJACKING              5630\n",
      "COMMANDINJECTION              5168\n",
      "SQLINJECTION                  5021\n",
      "XSS                           3705\n",
      "BACKDOOR_MALWARE              3078\n",
      "RECON-PINGSWEEP               2161\n",
      "UPLOADING_ATTACK              1196\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['Label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cfc647d-039a-4b90-b13c-c8cad9a14d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Unmapped labels:\n",
      "Series([], Name: count, dtype: int64)\n",
      "‚úÖ Sampled dataset shape: (3000000, 30)\n",
      "‚úÖ Distribution:\n",
      " BroadLabel\n",
      "DDOS        1000000\n",
      "NON-DDOS    1000000\n",
      "BENIGN      1000000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# üîπ Step 1: Map detailed attack types to broad classes\n",
    "label_map = {\n",
    "    'BENIGN': 'BENIGN',\n",
    "    'DDOS-ICMP_FLOOD': 'DDOS',\n",
    "    'DDOS-UDP_FLOOD': 'DDOS',\n",
    "    'DDOS-TCP_FLOOD': 'DDOS',\n",
    "    'DDOS-PSHACK_FLOOD': 'DDOS',\n",
    "    'DDOS-SYN_FLOOD': 'DDOS',\n",
    "    'DDOS-RSTFINFLOOD': 'DDOS',\n",
    "    'DDOS-SYNONYMOUSIP_FLOOD': 'DDOS',\n",
    "    'DDOS-ICMP_FRAGMENTATION': 'DDOS',\n",
    "    'DDOS-UDP_FRAGMENTATION': 'DDOS',\n",
    "    'DDOS-ACK_FRAGMENTATION': 'DDOS',\n",
    "    'DDOS-HTTP_FLOOD': 'DDOS',\n",
    "    'DDOS-SLOWLORIS': 'DDOS',\n",
    "    'DOS-UDP_FLOOD': 'NON-DDOS',\n",
    "    'DOS-TCP_FLOOD': 'NON-DDOS',\n",
    "    'DOS-SYN_FLOOD': 'NON-DDOS',\n",
    "    'DOS-HTTP_FLOOD': 'NON-DDOS',\n",
    "    'MIRAI-GREETH_FLOOD': 'NON-DDOS',\n",
    "    'MIRAI-UDPPLAIN': 'NON-DDOS',\n",
    "    'MIRAI-GREIP_FLOOD': 'NON-DDOS',\n",
    "    'VULNERABILITYSCAN': 'NON-DDOS',\n",
    "    'MITM-ARPSPOOFING': 'NON-DDOS',\n",
    "    'DNS_SPOOFING': 'NON-DDOS',\n",
    "    'RECON-HOSTDISCOVERY': 'NON-DDOS',\n",
    "    'RECON-OSSCAN': 'NON-DDOS',\n",
    "    'RECON-PORTSCAN': 'NON-DDOS',\n",
    "    'DICTIONARYBRUTEFORCE': 'NON-DDOS',\n",
    "    'BROWSERHIJACKING': 'NON-DDOS',\n",
    "    'COMMANDINJECTION': 'NON-DDOS',\n",
    "    'SQLINJECTION': 'NON-DDOS',\n",
    "    'XSS': 'NON-DDOS',\n",
    "    'BACKDOOR_MALWARE': 'NON-DDOS',\n",
    "    'RECON-PINGSWEEP': 'NON-DDOS',\n",
    "    'UPLOADING_ATTACK': 'NON-DDOS'\n",
    "}\n",
    "\n",
    "df['BroadLabel'] = df['Label'].map(label_map)\n",
    "\n",
    "# üîπ Step 2: Check for any unmapped labels\n",
    "if df['BroadLabel'].isnull().sum() > 0:\n",
    "    print(\"‚ö†Ô∏è Unmapped labels:\")\n",
    "    print(df[df['BroadLabel'].isnull()]['Label'].value_counts())\n",
    "\n",
    "# üîπ Step 3: Filter out null mappings\n",
    "df = df[df['BroadLabel'].notnull()]\n",
    "\n",
    "# üîπ Step 4: Sample 1 million from each major class\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "sampled_df = pd.DataFrame()\n",
    "for broad_class in ['BENIGN', 'DDOS', 'NON-DDOS']:\n",
    "    subset = df[df['BroadLabel'] == broad_class]\n",
    "    sample_n = min(1_000_000, len(subset))\n",
    "    sampled_subset = subset.sample(n=sample_n, random_state=42)\n",
    "    sampled_df = pd.concat([sampled_df, sampled_subset], axis=0)\n",
    "\n",
    "sampled_df = shuffle(sampled_df).reset_index(drop=True)\n",
    "print(\"‚úÖ Sampled dataset shape:\", sampled_df.shape)\n",
    "print(\"‚úÖ Distribution:\\n\", sampled_df['BroadLabel'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74498602-8a23-4413-9873-c9036be7cc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Shape after deduplication: (2323194, 30)\n"
     ]
    }
   ],
   "source": [
    "# üîπ Step 5: Remove duplicate rows based on feature columns\n",
    "# Keep only numeric or useful features (exclude Label/BroadLabel)\n",
    "feature_columns = sampled_df.select_dtypes(include=['number']).columns.tolist()\n",
    "dedup_df = sampled_df.drop_duplicates(subset=feature_columns)\n",
    "print(\"‚úÖ Shape after deduplication:\", dedup_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba536b65-c4e4-4d1c-adfb-693a67a3c0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Dropping 0 low-variance or null-heavy columns: []\n"
     ]
    }
   ],
   "source": [
    "# üîπ Step 6: Drop columns with all 0/null or too many nulls\n",
    "threshold = 0.98  # if more than 98% are 0 or NaN\n",
    "to_drop = []\n",
    "\n",
    "for col in feature_columns:\n",
    "    if sampled_df[col].isnull().mean() > threshold or (sampled_df[col] == 0).mean() > threshold:\n",
    "        to_drop.append(col)\n",
    "\n",
    "print(f\"‚ö†Ô∏è Dropping {len(to_drop)} low-variance or null-heavy columns: {to_drop}\")\n",
    "clean_df = dedup_df.drop(columns=to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33bec960-4d18-49a9-ba35-f7a484b4addb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Log normalization applied.\n",
      "‚úÖ Final preprocessed dataset shape: (2323194, 30)\n"
     ]
    }
   ],
   "source": [
    "# üîπ Step 7: Log normalization (on numeric columns only)\n",
    "import numpy as np\n",
    "\n",
    "numeric_cols = clean_df.select_dtypes(include=['number']).columns\n",
    "for col in numeric_cols:\n",
    "    clean_df[col] = clean_df[col].apply(lambda x: np.log1p(x))  # log1p handles 0 safely\n",
    "\n",
    "print(\"‚úÖ Log normalization applied.\")\n",
    "print(\"‚úÖ Final preprocessed dataset shape:\", clean_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa8627d3-33b6-460c-8e78-6b8aa970c460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset saved to '0final_cleaned_dataset.csv' with shape (2323194, 30)\n"
     ]
    }
   ],
   "source": [
    "# üîπ Save the final cleaned and normalized dataset to CSV\n",
    "output_path = \"0final_cleaned_dataset.csv\"\n",
    "clean_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Dataset saved to '{output_path}' with shape {clean_df.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
