{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "304c0ac7-2ed1-4b97-bd0b-47add566a99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total combined data shape: (45019243, 40)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Set the directory containing your CSVs\n",
    "data_dir = '/Users/user/Downloads/Table/downloaded_csvs_merge'  # <-- update this if needed\n",
    "all_dfs = []\n",
    "\n",
    "# Load each CSV file into a list of DataFrames\n",
    "for file in os.listdir(data_dir):\n",
    "    if file.endswith(\".csv\"):\n",
    "        try:\n",
    "            df = pd.read_csv(os.path.join(data_dir, file))\n",
    "            all_dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "# Combine all DataFrames\n",
    "df = pd.concat(all_dfs, ignore_index=True)\n",
    "print(\"Total combined data shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b727cd19-9882-4fc3-a624-9459a7a3abba",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.95\u001b[39m\n\u001b[1;32m      3\u001b[0m drop_cols \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m df[col]\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;241m>\u001b[39m threshold \u001b[38;5;129;01mor\u001b[39;00m (df[col] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;241m>\u001b[39m threshold:\n\u001b[1;32m      7\u001b[0m         drop_cols\u001b[38;5;241m.\u001b[39mappend(col)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Drop columns where more than 95% of values are NaN or 0\n",
    "threshold = 0.95\n",
    "drop_cols = []\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].isnull().mean() > threshold or (df[col] == 0).mean() > threshold:\n",
    "        drop_cols.append(col)\n",
    "\n",
    "df.drop(columns=drop_cols, inplace=True)\n",
    "print(\"Shape after dropping low-signal features:\", df.shape)\n",
    "\n",
    "# Drop rows where 'Label' is missing\n",
    "df.dropna(subset=['Label'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed88952e-02bf-44f9-aad7-ef43a1903acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Data directory set to: /Users/user/Downloads/Table/downloaded_csvs_merge\n",
      "Step 2: Initialized empty list for DataFrames.\n",
      "Step 3: Loading CSV files...\n",
      "  - Attempting to read: Merged27.csv\n",
      "    Successfully loaded: Merged27.csv | Shape: (720736, 40)\n",
      "  - Attempting to read: Merged33.csv\n",
      "    Successfully loaded: Merged33.csv | Shape: (793945, 40)\n",
      "  - Attempting to read: Merged32.csv\n",
      "    Successfully loaded: Merged32.csv | Shape: (704067, 40)\n",
      "  - Attempting to read: Merged26.csv\n",
      "    Successfully loaded: Merged26.csv | Shape: (878940, 40)\n",
      "  - Attempting to read: Merged18.csv\n",
      "    Successfully loaded: Merged18.csv | Shape: (714063, 40)\n",
      "  - Attempting to read: Merged30.csv\n",
      "    Successfully loaded: Merged30.csv | Shape: (695672, 40)\n",
      "  - Attempting to read: Merged24.csv\n",
      "    Successfully loaded: Merged24.csv | Shape: (743201, 40)\n",
      "  - Attempting to read: Merged25.csv\n",
      "    Successfully loaded: Merged25.csv | Shape: (700455, 40)\n",
      "  - Attempting to read: Merged31.csv\n",
      "    Successfully loaded: Merged31.csv | Shape: (682595, 40)\n",
      "  - Attempting to read: Merged19.csv\n",
      "    Successfully loaded: Merged19.csv | Shape: (701978, 40)\n",
      "  - Attempting to read: Merged35.csv\n",
      "    Successfully loaded: Merged35.csv | Shape: (920543, 40)\n",
      "  - Attempting to read: Merged21.csv\n",
      "    Successfully loaded: Merged21.csv | Shape: (692566, 40)\n",
      "  - Attempting to read: Merged09.csv\n",
      "    Successfully loaded: Merged09.csv | Shape: (678882, 40)\n",
      "  - Attempting to read: Merged08.csv\n",
      "    Successfully loaded: Merged08.csv | Shape: (712220, 40)\n",
      "  - Attempting to read: Merged20.csv\n",
      "    Successfully loaded: Merged20.csv | Shape: (719927, 40)\n",
      "  - Attempting to read: Merged34.csv\n",
      "    Successfully loaded: Merged34.csv | Shape: (899094, 40)\n",
      "  - Attempting to read: Merged22.csv\n",
      "    Successfully loaded: Merged22.csv | Shape: (739382, 40)\n",
      "  - Attempting to read: Merged36.csv\n",
      "    Successfully loaded: Merged36.csv | Shape: (743302, 40)\n",
      "  - Attempting to read: Merged37.csv\n",
      "    Successfully loaded: Merged37.csv | Shape: (688865, 40)\n",
      "  - Attempting to read: Merged23.csv\n",
      "    Successfully loaded: Merged23.csv | Shape: (688470, 40)\n",
      "  - Attempting to read: Merged44.csv\n",
      "    Successfully loaded: Merged44.csv | Shape: (725937, 40)\n",
      "  - Attempting to read: Merged50.csv\n",
      "    Successfully loaded: Merged50.csv | Shape: (601085, 40)\n",
      "  - Attempting to read: Merged51.csv\n",
      "    Successfully loaded: Merged51.csv | Shape: (214394, 40)\n",
      "  - Attempting to read: Merged45.csv\n",
      "    Successfully loaded: Merged45.csv | Shape: (673548, 40)\n",
      "  - Attempting to read: Merged53.csv\n",
      "    Successfully loaded: Merged53.csv | Shape: (907087, 40)\n",
      "  - Attempting to read: Merged47.csv\n",
      "    Successfully loaded: Merged47.csv | Shape: (657626, 40)\n",
      "  - Attempting to read: Merged46.csv\n",
      "    Successfully loaded: Merged46.csv | Shape: (678328, 40)\n",
      "  - Attempting to read: Merged52.csv\n",
      "    Successfully loaded: Merged52.csv | Shape: (65724, 40)\n",
      "  - Attempting to read: Merged56.csv\n",
      "    Successfully loaded: Merged56.csv | Shape: (743836, 40)\n",
      "  - Attempting to read: Merged42.csv\n",
      "    Successfully loaded: Merged42.csv | Shape: (871730, 40)\n",
      "  - Attempting to read: Merged43.csv\n",
      "    Successfully loaded: Merged43.csv | Shape: (692217, 40)\n",
      "  - Attempting to read: Merged57.csv\n",
      "    Successfully loaded: Merged57.csv | Shape: (865303, 40)\n",
      "  - Attempting to read: Merged41.csv\n",
      "    Successfully loaded: Merged41.csv | Shape: (712847, 40)\n",
      "  - Attempting to read: Merged55.csv\n",
      "    Successfully loaded: Merged55.csv | Shape: (754784, 40)\n",
      "  - Attempting to read: Merged54.csv\n",
      "    Successfully loaded: Merged54.csv | Shape: (727442, 40)\n",
      "  - Attempting to read: Merged40.csv\n",
      "    Successfully loaded: Merged40.csv | Shape: (696694, 40)\n",
      "  - Attempting to read: Merged59.csv\n",
      "    Successfully loaded: Merged59.csv | Shape: (687179, 40)\n",
      "  - Attempting to read: Merged58.csv\n",
      "    Successfully loaded: Merged58.csv | Shape: (709636, 40)\n",
      "  - Attempting to read: Merged63.csv\n",
      "    Successfully loaded: Merged63.csv | Shape: (428161, 40)\n",
      "  - Attempting to read: Merged62.csv\n",
      "    Successfully loaded: Merged62.csv | Shape: (701507, 40)\n",
      "  - Attempting to read: Merged60.csv\n",
      "    Successfully loaded: Merged60.csv | Shape: (691573, 40)\n",
      "  - Attempting to read: Merged48.csv\n",
      "    Successfully loaded: Merged48.csv | Shape: (657630, 40)\n",
      "  - Attempting to read: Merged49.csv\n",
      "    Successfully loaded: Merged49.csv | Shape: (642736, 40)\n",
      "  - Attempting to read: Merged61.csv\n",
      "    Successfully loaded: Merged61.csv | Shape: (711577, 40)\n",
      "  - Attempting to read: Merged06.csv\n",
      "    Successfully loaded: Merged06.csv | Shape: (718229, 40)\n",
      "  - Attempting to read: Merged12.csv\n",
      "    Successfully loaded: Merged12.csv | Shape: (896941, 40)\n",
      "  - Attempting to read: Merged13.csv\n",
      "    Successfully loaded: Merged13.csv | Shape: (709937, 40)\n",
      "  - Attempting to read: Merged07.csv\n",
      "    Successfully loaded: Merged07.csv | Shape: (702212, 40)\n",
      "  - Attempting to read: Merged39.csv\n",
      "    Successfully loaded: Merged39.csv | Shape: (677113, 40)\n",
      "  - Attempting to read: Merged11.csv\n",
      "    Successfully loaded: Merged11.csv | Shape: (909761, 40)\n",
      "  - Attempting to read: Merged05.csv\n",
      "    Successfully loaded: Merged05.csv | Shape: (744804, 40)\n",
      "  - Attempting to read: Merged04.csv\n",
      "    Successfully loaded: Merged04.csv | Shape: (676620, 40)\n",
      "  - Attempting to read: Merged10.csv\n",
      "    Successfully loaded: Merged10.csv | Shape: (916190, 40)\n",
      "  - Attempting to read: Merged38.csv\n",
      "    Successfully loaded: Merged38.csv | Shape: (729577, 40)\n",
      "  - Attempting to read: Merged14.csv\n",
      "    Successfully loaded: Merged14.csv | Shape: (902920, 40)\n",
      "  - Attempting to read: Merged28.csv\n",
      "    Successfully loaded: Merged28.csv | Shape: (706379, 40)\n",
      "  - Attempting to read: Merged29.csv\n",
      "    Successfully loaded: Merged29.csv | Shape: (717507, 40)\n",
      "  - Attempting to read: Merged01.csv\n",
      "    Successfully loaded: Merged01.csv | Shape: (712311, 40)\n",
      "  - Attempting to read: Merged15.csv\n",
      "    Successfully loaded: Merged15.csv | Shape: (718483, 40)\n",
      "  - Attempting to read: Merged03.csv\n",
      "    Successfully loaded: Merged03.csv | Shape: (697289, 40)\n",
      "  - Attempting to read: Merged17.csv\n",
      "    Successfully loaded: Merged17.csv | Shape: (731532, 40)\n",
      "  - Attempting to read: Merged16.csv\n",
      "    Successfully loaded: Merged16.csv | Shape: (765369, 40)\n",
      "  - Attempting to read: Merged02.csv\n",
      "    Successfully loaded: Merged02.csv | Shape: (748585, 40)\n",
      "Step 4: Concatenating all DataFrames...\n",
      "Step 5: Total combined data shape: (45019243, 40)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Step 1: Set the directory containing your CSVs\n",
    "data_dir = '/Users/user/Downloads/Table/downloaded_csvs_merge'  # <-- update this if needed\n",
    "print(\"Step 1: Data directory set to:\", data_dir)\n",
    "\n",
    "# Step 2: Initialize an empty list to store DataFrames\n",
    "all_dfs = []\n",
    "print(\"Step 2: Initialized empty list for DataFrames.\")\n",
    "\n",
    "# Step 3: Load each CSV file into a list of DataFrames\n",
    "print(\"Step 3: Loading CSV files...\")\n",
    "for file in os.listdir(data_dir):\n",
    "    if file.endswith(\".csv\"):\n",
    "        file_path = os.path.join(data_dir, file)\n",
    "        print(f\"  - Attempting to read: {file}\")\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            all_dfs.append(df)\n",
    "            print(f\"    Successfully loaded: {file} | Shape: {df.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Error reading {file}: {e}\")\n",
    "\n",
    "# Step 4: Combine all DataFrames\n",
    "print(\"Step 4: Concatenating all DataFrames...\")\n",
    "if all_dfs:\n",
    "    df = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(\"Step 5: Total combined data shape:\", df.shape)\n",
    "else:\n",
    "    print(\"No CSV files were successfully loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d49f374-afaa-4718-926d-e8773b911f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after dropping low-signal features: (45019243, 32)\n"
     ]
    }
   ],
   "source": [
    "# Drop columns where more than 95% of values are NaN or 0\n",
    "threshold = 0.95\n",
    "drop_cols = []\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].isnull().mean() > threshold or (df[col] == 0).mean() > threshold:\n",
    "        drop_cols.append(col)\n",
    "\n",
    "df.drop(columns=drop_cols, inplace=True)\n",
    "print(\"Shape after dropping low-signal features:\", df.shape)\n",
    "\n",
    "# Drop rows where 'Label' is missing\n",
    "df.dropna(subset=['Label'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1a0d3e5-4374-48f0-9c22-0cf35af89ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns kept: ['Header_Length', 'Protocol Type', 'Time_To_Live', 'Rate', 'fin_flag_number', 'syn_flag_number', 'rst_flag_number', 'psh_flag_number', 'ack_flag_number', 'ack_count', 'syn_count', 'fin_count', 'rst_count', 'HTTP', 'HTTPS', 'DNS', 'TCP', 'UDP', 'ARP', 'ICMP', 'IPv', 'LLC', 'Tot sum', 'Min', 'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number', 'Variance', 'Label']\n"
     ]
    }
   ],
   "source": [
    "# Keep only numeric columns and the 'Label' column\n",
    "numeric_features = df.select_dtypes(include='number').columns.tolist()\n",
    "if 'Label' in df.columns:\n",
    "    numeric_features += ['Label']\n",
    "\n",
    "df = df[numeric_features]\n",
    "df['Label'] = df['Label'].astype(str)\n",
    "\n",
    "print(\"Columns kept:\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbcb8106-0dd0-4703-b614-f435cdbb80da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v4/gtwpdh8d1xbgyt5jg7_bm78m0000gn/T/ipykernel_5700/3568844501.py:11: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(min(1_000_000, len(x)), random_state=42))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled major groups shape: (1000000, 33)\n",
      "Major class distribution:\n",
      " ClassGroup\n",
      "Non-DDoS    1000000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create major class group\n",
    "df['ClassGroup'] = df['Label'].apply(lambda x: (\n",
    "    'BenignTraffic' if x == 'BenignTraffic' else\n",
    "    'Non-DDoS' if not x.startswith('DDoS') else\n",
    "    'DDoS'\n",
    "))\n",
    "\n",
    "# Sample up to 1,000,000 from each major group\n",
    "sampled_major = (\n",
    "    df.groupby('ClassGroup')\n",
    "    .apply(lambda x: x.sample(min(1_000_000, len(x)), random_state=42))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"Sampled major groups shape:\", sampled_major.shape)\n",
    "print(\"Major class distribution:\\n\", sampled_major['ClassGroup'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5864bea8-c68d-4500-96bf-cb95fd4fbc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 269381 duplicates\n"
     ]
    }
   ],
   "source": [
    "before_dedup = sampled_major.shape[0]\n",
    "sampled_major = sampled_major.drop_duplicates()\n",
    "after_dedup = sampled_major.shape[0]\n",
    "\n",
    "print(f\"Removed {before_dedup - after_dedup} duplicates\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7033cc5c-bb3f-4a6a-bed9-112c2ed6ebf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDoS subclass distribution before dedup:\n",
      " Series([], Name: count, dtype: int64)\n",
      "DDoS subclass distribution after dedup:\n",
      " Series([], Name: count, dtype: int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v4/gtwpdh8d1xbgyt5jg7_bm78m0000gn/T/ipykernel_5700/1541566698.py:7: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(min(100_000, len(x)), random_state=42))\n"
     ]
    }
   ],
   "source": [
    "# Filter DDoS class\n",
    "ddos_df = df[df['ClassGroup'] == 'DDoS']\n",
    "\n",
    "# Sample up to 100,000 from each DDoS subclass\n",
    "ddos_subsampled = (\n",
    "    ddos_df.groupby('Label')\n",
    "    .apply(lambda x: x.sample(min(100_000, len(x)), random_state=42))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "ddos_dedup = ddos_subsampled.drop_duplicates()\n",
    "\n",
    "print(\"DDoS subclass distribution before dedup:\\n\", ddos_subsampled['Label'].value_counts())\n",
    "print(\"DDoS subclass distribution after dedup:\\n\", ddos_dedup['Label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "61da6a82-842a-4513-9cd3-24064900801e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved cleaned datasets:\n",
      "  • CICIoT2023_major_balanced_cleaned.csv\n",
      "  • CICIoT2023_ddos_subclasses_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# === Step 7: Log Normalization (excluding Label columns) ===\n",
    "def log_normalize(df):\n",
    "    \"\"\"\n",
    "    Apply log-normalization to numeric columns in the DataFrame.\n",
    "    \"\"\"\n",
    "    df_numeric = df.select_dtypes(include='number')\n",
    "    return df_numeric.apply(lambda x: np.log1p(x))  # log(1 + x)\n",
    "\n",
    "# Normalize the major class balanced dataset\n",
    "features = sampled_major.drop(columns=['Label', 'ClassGroup'])\n",
    "labels = sampled_major[['Label']]\n",
    "features_normalized = log_normalize(features)\n",
    "processed_major = pd.concat([features_normalized, labels], axis=1)\n",
    "\n",
    "# Normalize the DDoS subclasses dataset\n",
    "features_ddos = ddos_dedup.drop(columns=['Label', 'ClassGroup'])\n",
    "labels_ddos = ddos_dedup[['Label']]\n",
    "features_ddos_norm = log_normalize(features_ddos)\n",
    "processed_ddos = pd.concat([features_ddos_norm, labels_ddos], axis=1)\n",
    "\n",
    "# === Save final outputs ===\n",
    "processed_major.to_csv(\"CICIoT2023_major_balanced_cleaned.csv\", index=False)\n",
    "processed_ddos.to_csv(\"CICIoT2023_ddos_subclasses_cleaned.csv\", index=False)\n",
    "\n",
    "print(\"✅ Saved cleaned datasets:\")\n",
    "print(\"  • CICIoT2023_major_balanced_cleaned.csv\")\n",
    "print(\"  • CICIoT2023_ddos_subclasses_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c994f154-9742-4273-8e95-fc46b500d5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved cleaned datasets:\n",
      "  • CICIoT2023_major_balanced_cleaned.csv\n",
      "  • CICIoT2023_ddos_subclasses_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "processed_major.to_csv(\"CICIoT2023_major_balanced_cleaned.csv\", index=False)\n",
    "processed_ddos.to_csv(\"CICIoT2023_ddos_subclasses_cleaned.csv\", index=False)\n",
    "\n",
    "print(\"✅ Saved cleaned datasets:\")\n",
    "print(\"  • CICIoT2023_major_balanced_cleaned.csv\")\n",
    "print(\"  • CICIoT2023_ddos_subclasses_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10798e28-a67f-49ee-8083-7901f9fdb1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved cleaned datasets:\n",
      "  • /Users/user/Downloads/Table/CICIoT2023_major_balanced_cleaned.csv\n",
      "  • /Users/user/Downloads/Table/CICIoT2023_ddos_subclasses_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# Set your target directory\n",
    "save_dir = \"/Users/user/Downloads/Table\"  # Replace with your actual directory path\n",
    "\n",
    "# Save the files\n",
    "processed_major.to_csv(f\"{save_dir}/CICIoT2023_major_balanced_cleaned.csv\", index=False)\n",
    "processed_ddos.to_csv(f\"{save_dir}/CICIoT2023_ddos_subclasses_cleaned.csv\", index=False)\n",
    "\n",
    "print(\"✅ Saved cleaned datasets:\")\n",
    "print(f\"  • {save_dir}/CICIoT2023_major_balanced_cleaned.csv\")\n",
    "print(f\"  • {save_dir}/CICIoT2023_ddos_subclasses_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f9d2d921-ab33-446b-a8b6-48dd63b2ad51",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m X_train_major, X_test_major, y_train_major, y_test_major \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m      5\u001b[0m     features_normalized, labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Split the processed DDoS dataset into train and test sets (80% train, 20% test)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m X_train_ddos, X_test_ddos, y_train_ddos, y_test_ddos \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m     10\u001b[0m     features_ddos_norm, labels_ddos, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining set (Major Classes): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train_major\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test set (Major Classes): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_test_major\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining set (DDoS subclasses): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train_ddos\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test set (DDoS subclasses): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_test_ddos\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_split.py:2660\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2657\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[1;32m   2659\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m-> 2660\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[1;32m   2661\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[1;32m   2662\u001b[0m )\n\u001b[1;32m   2664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m   2665\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_split.py:2308\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2305\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[1;32m   2307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2309\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2310\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2311\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[1;32m   2312\u001b[0m     )\n\u001b[1;32m   2314\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the processed major dataset into train and test sets (80% train, 20% test)\n",
    "X_train_major, X_test_major, y_train_major, y_test_major = train_test_split(\n",
    "    features_normalized, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Split the processed DDoS dataset into train and test sets (80% train, 20% test)\n",
    "X_train_ddos, X_test_ddos, y_train_ddos, y_test_ddos = train_test_split(\n",
    "    features_ddos_norm, labels_ddos, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set (Major Classes): {X_train_major.shape}, Test set (Major Classes): {X_test_major.shape}\")\n",
    "print(f\"Training set (DDoS subclasses): {X_train_ddos.shape}, Test set (DDoS subclasses): {X_test_ddos.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d574120-6f76-4f70-bf90-2d756ecbc36a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
